{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhammed/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:87: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/home/muhammed/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:137: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "/home/muhammed/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:159: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "/home/muhammed/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:167: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  INF = pd.np.inf\n",
      "/home/muhammed/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:168: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  NAN = pd.np.nan\n",
      "/home/muhammed/anaconda3/lib/python3.7/site-packages/pugnlp/tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/home/muhammed/anaconda3/lib/python3.7/site-packages/pugnlp/util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/home/muhammed/anaconda3/lib/python3.7/site-packages/nlpia/futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/home/muhammed/anaconda3/lib/python3.7/site-packages/nlpia/loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nlpia.loaders import get_data\n",
    "\n",
    "EMBEDDING_FILE = '/home/muhammed/Documents/nlpia_codes/GoogleNews-vectors-negative300.bin' # from above\n",
    "word_vectors = KeyedVectors.load_word2vec_format(EMBEDDING_FILE,limit = 20000, binary=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(file_path):\n",
    "    positive_path = os.path.join(file_path, 'pos')\n",
    "    negative_path = os.path.join(file_path, 'neg')\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    dataset = []\n",
    "\n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "    for filename in glob.glob(os.path.join(negative_path,\"*.txt\")):\n",
    "        with open(filename) as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "    shuffle(dataset)\n",
    "\n",
    "    return dataset\n",
    "train_dataset = pre_process_data('/home/muhammed/Documents/nlpia_codes/ch7/aclImdb/train')\n",
    "test_dataset = pre_process_data('/home/muhammed/Documents/nlpia_codes/ch7/aclImdb/test')\n",
    "dataset = train_dataset + test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "   \n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        #the document is tokenized into sentences and sentences into tokens\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except:\n",
    "                pass #No matching token in the Google w2v\n",
    "        vectorized_data.append(sample_vecs)\n",
    "    return vectorized_data\n",
    "\n",
    "\n",
    "def collect_expected(dataset):\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_length(data):\n",
    "    total_len = 0\n",
    "    for sample in data:\n",
    "        total_len += len(sample[1])\n",
    "    return total_len / len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "split_point = int(len(vectorized_data)*0.8)\n",
    "x_train = vectorized_data[:split_point]\n",
    "#note take the length of the vectorized_data because maybe len(vecotized) != len(dataset)\n",
    "#since we only using limited version of GoogleEmbeddings\n",
    "y_train = expected[:split_point]\n",
    "\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data,maxlen):\n",
    "    #pad dataset with zero embedding vectors and truncate the longer one\n",
    "    new_data = []\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = pad_trunc(x_train, max_len)\n",
    "x_test = pad_trunc(x_test, max_len)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), max_len, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), max_len, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN, LSTM\n",
    "num_neurons = 50\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(\n",
    "            num_neurons, return_sequences = True,\n",
    "            input_shape = (max_len, embedding_dims)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-19 13:45:13,499 WARNING:     tensorflow:323:            new_func From /home/muhammed/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile('rmsprop','binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5001      \n",
      "=================================================================\n",
      "Total params: 75,201\n",
      "Trainable params: 75,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e8b4f75b99e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m          \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          validation_data = (x_test, y_test))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "         batch_size = batch_size,\n",
    "         epochs = epochs,\n",
    "         validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-7a0e9eaa5ec6>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-7a0e9eaa5ec6>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    for char sample[1].lower():\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def clean_data(data):\n",
    "    new_data = []\n",
    "    VALID = 'abcdefghijklmnopqrstuvwxyz0123456789\"\\'?!.,:; '\n",
    "    for sample in data:\n",
    "        new_sample = []\n",
    "        for char sample[1].lower():\n",
    "            if char in VALID:\n",
    "                new_sample.append(char)\n",
    "            else:\n",
    "                new_sample.append('UNK')\n",
    "    return new_data\n",
    "listed_data = clean_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_pad_trunc(data, maxlen = 1500):\n",
    "    new_dataset = []\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            new_data = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            pads = maxlen - len(sample)\n",
    "            new_data = sample + ['PAD']*pads\n",
    "        else:\n",
    "            new_data = sample\n",
    "        new_dataset.append(new_data)\n",
    "    return new_dataset\n",
    "\n",
    "#this time instead of working with word2vec embedding you are going to work with one_hot vectors to encode the character\n",
    "# we will have two dictionaries one maps from token into index and the other maps from index into token\n",
    "def create_dicts(data):\n",
    "    chars = set()\n",
    "    for sample in data:\n",
    "        chars.update(set(sample))\n",
    "    char_indices = dict((c,i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i,c) for i, c in enumerate(chars))\n",
    "    return char_indices, indices_char\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def onehot_encoding(dataset, char_indices, maxlen  =1500):\n",
    "    X = np.zeros((len(dataset), maxlen, len(char_indices.keys())))\n",
    "    for i, sentence in enumerate(dataset):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i,t, char_indices[char]] = 1\n",
    "    return X\n",
    "#onehot_encoding numpy array of length equal to the number of data samples, each sample with length maxlen\n",
    "#and each token will be one-hot encoded vector of length equal to the number of characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_data = clean_data(dataset)\n",
    "#this step the dataset will be converted from word tokens into char tokens\n",
    "common_length_data = char_pad_trunc(listed_data, maxlen = 1500)\n",
    "#here the data will be padded\n",
    "char_indices, indices_char = create_dicts(common_length_data)\n",
    "#dictionary for char_indices, indices_char used in encoding the char tokens\n",
    "encoded_data = onehot_encode(common_length_data)\n",
    "#numpy array with lenght equals to the total samples of the dataset, each with length maxlen and each char of the data with lenght equal to the number of tokens\n",
    "split_point = int(len(encoded_data)*0.8)\n",
    "\n",
    "x_train = encoded_data[:split_point]\n",
    "x_test = encoded_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "y_test = expected[split_point:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons,\n",
    "              return_sequences = True,\n",
    "              input_shape = (maxlen, len(char_indices.keys()))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics = ['accuarcy'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
